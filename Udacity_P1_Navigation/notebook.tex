
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Navigation\_DQN\_Assigment}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{daniel-bain}{%
\section{Daniel Bain}\label{daniel-bain}}

\hypertarget{udacity-deep-reinforcement-learning-course}{%
\section{Udacity Deep Reinforcement Learning
Course}\label{udacity-deep-reinforcement-learning-course}}

\hypertarget{project-1-navigation}{%
\section{Project \#1: Navigation}\label{project-1-navigation}}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this notebook, I am utilizing a Deep Q Network (``DQN'') to apply
Reinforcement Learning to navigate a Unity ML-Agents environment for the
first project of the
\href{https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893}{Deep
Reinforcement Learning Nanodegree}.

To keep things simple and easier to follow along, all code except for
the game library engine is in this notebook.

\hypertarget{find-the-yellow-bananas}{%
\subsection{Find the Yellow Bananas}\label{find-the-yellow-bananas}}

For this assignment, we have been tasked with writing a Deep Q Network
to learn to play a 3-D navigation game to run over as many yellow
bananas as possible while avoiding blue bananas. This Deep Q Network is
generalized and can be applied to learn to play other games or solve
other tasks.

The banana game simulation contains a single agent that navigates the
environment. At each time step, it has four actions at its disposal:

\begin{itemize}
\tightlist
\item
  \texttt{0} - walk forward
\item
  \texttt{1} - walk backward
\item
  \texttt{2} - turn left
\item
  \texttt{3} - turn right
\end{itemize}

The state space has 37 dimensions and contains the agent's velocity,
along with ray-based perception of objects around agent's forward
direction.

A player earns a reward of \texttt{+1} for ``collecting'' (running into)
each yellow banana and \texttt{-1} for collecting each blue banana.

The assignment is to set up a value network to train to play the game
and successfully get an average of 13 points over 100 games.

\hypertarget{deep-q-network-dqn}{%
\subsection{Deep Q Network (DQN)}\label{deep-q-network-dqn}}

Deep Mind, now a subsidiary of Google, first demonstrated a Deep Q
Network to play Atari Games at the end of 2014 (Playing Atari with Deep
Reinforcement Learning, Minh et al.) and in more detail in a Nature
Article published in 2015. This was a major breakthrough because a
single network was able to learn to play multiple games without
modification and was able to play at human champion levels.

A Q function is a state-action value function that specifies how good an
action (``a'') is in the state (``s''). The value of all possible
actions in each state is stored in a table called a Q table. The Q table
is used to choose the optimal action (that with the maximum value) in a
state.

A deep Q Network uses a deep neural network to approximate the value
function.

Unlike in the Deep Mind papers, where they used screen shots of games to
train the network, for the first phase of this assignment have been
asked to use the above described 37 dimension state space as inputs to
train our network. This lower dimensioned state space is much simpler
and faster to train and does not require any preprocessing (outputted
values already appear to be normalized for easy training with a neural
network).

To complete this assignment, I have modified code from a vanilla DQN
used to train playing a Lunar Lander game that was presented earlier in
the Udacity course. Then I have implemented a couple simple improvements
by adding a Double DQN and a Dueling DQN, which are described in more
detail below.

\hypertarget{improvements-double-dqn-dueling-dqn}{%
\subsection{Improvements: Double DQN \& Dueling
DQN}\label{improvements-double-dqn-dueling-dqn}}

Since Deep Mind published its Atari papers in 2014 and 2015 there have
been a succession of improvements to the vanilla DQN model to help
improve learning and reach convergence faster.

In this notebook, I have attempted to implement two of these
improvements: (i) Double DQN; and (ii) Dueling DQN. Honestly, I chose
these two because they were fairly straight forward to implement.

\hypertarget{i-double-dqn}{%
\subsubsection{(i) Double DQN}\label{i-double-dqn}}

Conventional Q-learning suffers from overestimation bias in the
maximization step. Double Q-learning addresses this overestimation by
decoupling, in the maximization performed for the bootstrap target, the
selection of the action from its evaluation. (van Hasselt 2010) This
change was shown to reduce harmful overestimations that were present for
DQN, thereby improving performance.

\hypertarget{ii-dueling-dqn}{%
\subsubsection{(ii) Dueling DQN}\label{ii-dueling-dqn}}

Dueling DQN's were proposed in a 2016 Deep Mind paper
(arXiv:1511.06581v3 {[}cs.LG{]} 5 Apr 2016). The dueling network
represents two separate estimators: one for the state value function and
one for the state-dependent action advantage function. The researchers
at Deep Mind showed that by doing so, learning is generalized across
actions without imposing any change to the underlying reinforcement
learning algorithm. They also showed better learning and faster
convergence.

The figure below from the Deep Mind paper illustrates a basic DQN (top)
and a dueling architecture (bottom):

\begin{figure}
\centering
\includegraphics{attachment:image.png}
\caption{image.png}
\end{figure}

\hypertarget{other-improvements-to-dqn}{%
\subsubsection{Other Improvements to
DQN}\label{other-improvements-to-dqn}}

Deep Mind wrote another paper in October, 2017 where they showed that
combining 6 improvement techniques to the original DQN model
(https://arxiv.org/pdf/1710.02298.pdf) provided for faster
training/convergence as well as for higher performance. In addition to
the 2 techniques mentioned above and implemented herein, Deep Mind also
included (1) Prioritized DQN (e.g., prioritize samples from the
Experience Replay Buffer where there is more to learn), (2) Multi-Step
Learning (rather than simply looking ahead one step, forward view
multi-step targets are used), (3) Distributional RL (learn to
approximate distribution of returns rather than expected return), and
(4) Noisy Nets (noisy linear layer that combines a deterministic and
noisy stream).

Obviously, if you are coding a DQN, you'd probably start with code that
already implements all 6 of the ``rainbow'' of techniques\ldots{}

    \hypertarget{set-up}{%
\subsection{1. Set Up}\label{set-up}}

First step is to import needed libraries, start up the ``Unity'' (banana
game playing) environment, and determine the number of actions and size
of the state space.

I am running this code on my local Mac and therefore the banana game
Unity library is located locally on my Mac.

While I have included the Mac version of the Unity banana environment
that I ran on my local machine on this Github repo, if you want to run
this code on your local machine, Udacity provides the following links to
download the banana environment (obviously, select the environment that
matches your operating system):

\begin{itemize}
\tightlist
\item
  Linux:
  https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana\_Linux.zip
\item
  Mac OSX:
  https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana.app.zip
\item
  Windows (32-bit):
  https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana\_Windows\_x86.zip
\item
  Windows (64-bit):
  https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana\_Windows\_x86\_64.zip
\end{itemize}

Also the filepath in the coe below should be changed depending on the
location of the Unity environment:

\begin{itemize}
\tightlist
\item
  \textbf{Mac}: \texttt{"path/to/Banana.app"}
\item
  \textbf{Windows} (x86):
  \texttt{"path/to/Banana\_Windows\_x86/Banana.exe"}
\item
  \textbf{Windows} (x86\_64):
  \texttt{"path/to/Banana\_Windows\_x86\_64/Banana.exe"}
\item
  \textbf{Linux} (x86): \texttt{"path/to/Banana\_Linux/Banana.x86"}
\item
  \textbf{Linux} (x86\_64):
  \texttt{"path/to/Banana\_Linux/Banana.x86\_64"}
\item
  \textbf{Linux} (x86, headless):
  \texttt{"path/to/Banana\_Linux\_NoVis/Banana.x86"}
\item
  \textbf{Linux} (x86\_64, headless):
  \texttt{"path/to/Banana\_Linux\_NoVis/Banana.x86\_64"}
\end{itemize}

For instance, if you are using a Mac, then you downloaded
\texttt{Banana.app}. If this file is in the same folder as the notebook,
then the line below should appear as follows:

\begin{verbatim}
env = UnityEnvironment(file_name="Banana.app")
\end{verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{c+c1}{\PYZsh{} import needed libraries}
        
        \PY{k+kn}{from} \PY{n+nn}{unityagents} \PY{k}{import} \PY{n}{UnityEnvironment}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{random}
        \PY{k+kn}{import} \PY{n+nn}{torch}
        \PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn} \PY{k}{as} \PY{n+nn}{nn}
        \PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn}\PY{n+nn}{.}\PY{n+nn}{functional} \PY{k}{as} \PY{n+nn}{F}
        \PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{optim} \PY{k}{as} \PY{n+nn}{optim}
        \PY{k+kn}{from} \PY{n+nn}{collections} \PY{k}{import} \PY{n}{namedtuple}\PY{p}{,} \PY{n}{deque}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
        
        
        \PY{c+c1}{\PYZsh{} Start the environment}
        \PY{n}{env} \PY{o}{=} \PY{n}{UnityEnvironment}\PY{p}{(}\PY{n}{file\PYZus{}name}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/Users/danielbain/deep\PYZhy{}reinforcement\PYZhy{}learning/Udacity\PYZus{}P1\PYZus{}Navigation/Banana.app}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} get the default brain (e.g., responsible for deciding the actions of associated agents)}
        \PY{n}{brain\PYZus{}name} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{brain\PYZus{}names}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        \PY{n}{brain} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{brains}\PY{p}{[}\PY{n}{brain\PYZus{}name}\PY{p}{]}
        
        \PY{c+c1}{\PYZsh{} reset the environment}
        \PY{n}{env\PYZus{}info} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{reset}\PY{p}{(}\PY{n}{train\PYZus{}mode}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{[}\PY{n}{brain\PYZus{}name}\PY{p}{]}
        
        \PY{c+c1}{\PYZsh{} determine number of actions \PYZam{} size of state}
        \PY{n}{action\PYZus{}size} \PY{o}{=} \PY{n}{brain}\PY{o}{.}\PY{n}{vector\PYZus{}action\PYZus{}space\PYZus{}size}
        \PY{n}{state} \PY{o}{=} \PY{n}{env\PYZus{}info}\PY{o}{.}\PY{n}{vector\PYZus{}observations}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        \PY{n}{state\PYZus{}size} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{state}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} use GPU if available, otherwise, use CPU}
        \PY{n}{device} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{device}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cuda:0}\PY{l+s+s2}{\PYZdq{}} \PY{k}{if} \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{is\PYZus{}available}\PY{p}{(}\PY{p}{)} \PY{k}{else} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cpu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
INFO:unityagents:
'Academy' started successfully!
Unity Academy name: Academy
        Number of Brains: 1
        Number of External Brains : 1
        Lesson number : 0
        Reset Parameters :
		
Unity brain name: BananaBrain
        Number of Visual Observations (per agent): 0
        Vector Observation space type: continuous
        Vector Observation space size (per agent): 37
        Number of stacked Vector Observation: 1
        Vector Action space type: discrete
        Vector Action space size (per agent): 4
        Vector Action descriptions: , , , 

    \end{Verbatim}

    \hypertarget{deep-learning-q-network-class}{%
\subsection{2. Deep Learning Q Network
Class}\label{deep-learning-q-network-class}}

Next step is to set up the Deep Learning Q Network class. Here I'm using
a simple two layer network in Pytorch with 64 nodes at each layer. I've
added 20\% dropout at each layer to reduce over-fitting and, at least
for a simple DQN, note improved performance with the dropout layers.

The Deep Q network takes the ``state size'' (e.g., parameters describing
what's going on) and ``action size'' (e.g., actions that can be taken)
as input and output parameters so the network is set up to the right
shape for the task at hand.

As with the approach Deep Mind utilized in their Nature paper, all the
Q-values are calculated in parallel with one pass through the network.
(Really wonderful!) This is much faster than than treating each Q(s, a)
separately.

I have never worked with Pytorch before--but I am quite familiar with
Tensorflow and Keras (and countinue to read-up in order to understand
some of the new syntax used later in the agent).

\hypertarget{implementation-for-dueling-dqn}{%
\subsubsection{Implementation for Dueling
DQN}\label{implementation-for-dueling-dqn}}

In addition, I implement an option to use Dueling DQN in the code below.
Given that the original Dueling DQN network was placed after a
convolutional network and a fully connected RELU layer, I tried both
having an initial RELU layer shared by both the value and advantage
networks as well as splitting the two off the bat.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{c+c1}{\PYZsh{} Set up Q Network}
        
        \PY{k}{class} \PY{n+nc}{QNetwork}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Actor (Policy) Model.\PYZdq{}\PYZdq{}\PYZdq{}}
        
            \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{state\PYZus{}size}\PY{p}{,} \PY{n}{action\PYZus{}size}\PY{p}{,} \PY{n}{seed}\PY{p}{,} \PY{n}{fc1\PYZus{}units}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{fc2\PYZus{}units}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{dueling}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Initialize parameters and build model.}
        \PY{l+s+sd}{        Params}
        \PY{l+s+sd}{        ======}
        \PY{l+s+sd}{            state\PYZus{}size (int): Dimension of each state}
        \PY{l+s+sd}{            action\PYZus{}size (int): Dimension of each action}
        \PY{l+s+sd}{            seed (int): Random seed}
        \PY{l+s+sd}{            fc1\PYZus{}units (int): Number of nodes in first hidden layer}
        \PY{l+s+sd}{            fc2\PYZus{}units (int): Number of nodes in second hidden layer}
        \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{n+nb}{super}\PY{p}{(}\PY{n}{QNetwork}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dueling} \PY{o}{=} \PY{n}{dueling}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{seed} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{manual\PYZus{}seed}\PY{p}{(}\PY{n}{seed}\PY{p}{)}
                \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dueling}\PY{p}{:}
                    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{            For a dueling network, instead of defining a single path }
        \PY{l+s+sd}{            of fully connected layers, the value and action components of }
        \PY{l+s+sd}{            the Bellman equation are approximated separately via}
        \PY{l+s+sd}{            2 different transformations:}
        \PY{l+s+sd}{            (1) one for value prediction; and }
        \PY{l+s+sd}{            (2) one for advantages }
        \PY{l+s+sd}{            \PYZdq{}\PYZdq{}\PYZdq{}}
                    \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc1} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{state\PYZus{}size}\PY{p}{,} \PY{n}{fc1\PYZus{}units}\PY{p}{)}
                    
        \PY{c+c1}{\PYZsh{}             self.val\PYZus{}fc1 = nn.Linear(state\PYZus{}size, fc1\PYZus{}units)}
                    \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{val\PYZus{}fc2} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{fc1\PYZus{}units}\PY{p}{,} \PY{n}{fc2\PYZus{}units}\PY{p}{)}
                    \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{val\PYZus{}fc3} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{fc2\PYZus{}units}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
                    
        \PY{c+c1}{\PYZsh{}             self.adv\PYZus{}fc1 = nn.Linear(state\PYZus{}size, fc1\PYZus{}units)}
                    \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{adv\PYZus{}fc2} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{fc1\PYZus{}units}\PY{p}{,} \PY{n}{fc2\PYZus{}units}\PY{p}{)}
                    \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{adv\PYZus{}fc3} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{fc2\PYZus{}units}\PY{p}{,} \PY{n}{action\PYZus{}size}\PY{p}{)}
                \PY{k}{else}\PY{p}{:}
                    \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc1} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{state\PYZus{}size}\PY{p}{,} \PY{n}{fc1\PYZus{}units}\PY{p}{)}
                    \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc2} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{fc1\PYZus{}units}\PY{p}{,} \PY{n}{fc2\PYZus{}units}\PY{p}{)}
                    \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc3} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{fc2\PYZus{}units}\PY{p}{,} \PY{n}{action\PYZus{}size}\PY{p}{)}
        
            \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{state}\PY{p}{)}\PY{p}{:}
                \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dueling}\PY{p}{:}
                    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{            For a dueling network, value and advantage are separated calculated for }
        \PY{l+s+sd}{            the batch of samples.  To determine Q\PYZhy{}values, value and advantage values }
        \PY{l+s+sd}{            are then added together, subtracting the mean of the advantages. }
        \PY{l+s+sd}{            \PYZdq{}\PYZdq{}\PYZdq{}}
                    \PY{c+c1}{\PYZsh{} fully connected layer prior to dueling network}
                    \PY{n}{x} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc1}\PY{p}{(}\PY{n}{state}\PY{p}{)}\PY{p}{)}
                    \PY{n}{x} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{dropout}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{training}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{training}\PY{p}{,} \PY{n}{p}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{)}
                    
                    \PY{c+c1}{\PYZsh{} value network}
        \PY{c+c1}{\PYZsh{}             val = F.relu(self.val\PYZus{}fc1(state))}
        \PY{c+c1}{\PYZsh{}             val = F.dropout(val, training=self.training, p=0.2)}
                    \PY{n}{val} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{val\PYZus{}fc2}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
                    \PY{n}{val} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{dropout}\PY{p}{(}\PY{n}{val}\PY{p}{,} \PY{n}{training}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{training}\PY{p}{,} \PY{n}{p}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{)}
                    \PY{n}{val} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{val\PYZus{}fc3}\PY{p}{(}\PY{n}{val}\PY{p}{)}
                    
                    \PY{c+c1}{\PYZsh{} advantage network}
        \PY{c+c1}{\PYZsh{}             adv = F.relu(self.adv\PYZus{}fc1(state))}
        \PY{c+c1}{\PYZsh{}             adv = F.dropout(adv, training=self.training, p=0.2)}
                    \PY{n}{adv} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{adv\PYZus{}fc2}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
                    \PY{n}{adv} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{dropout}\PY{p}{(}\PY{n}{adv}\PY{p}{,} \PY{n}{training}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{training}\PY{p}{,} \PY{n}{p}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{)}
                    \PY{n}{adv} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{adv\PYZus{}fc3}\PY{p}{(}\PY{n}{adv}\PY{p}{)}
                    
                    \PY{c+c1}{\PYZsh{} combine value and advantage networks}
                    \PY{n}{x} \PY{o}{=} \PY{n}{val} \PY{o}{+} \PY{n}{adv} \PY{o}{\PYZhy{}} \PY{n}{adv}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
                    
                \PY{k}{else}\PY{p}{:}
                    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{            Non\PYZhy{}Dueling Network: Build a network that maps state \PYZhy{}\PYZgt{} action values.}
        \PY{l+s+sd}{            \PYZdq{}\PYZdq{}\PYZdq{}}
                    \PY{n}{x} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc1}\PY{p}{(}\PY{n}{state}\PY{p}{)}\PY{p}{)}
                    \PY{n}{x} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{dropout}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{training}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{training}\PY{p}{,} \PY{n}{p}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{)}
                    \PY{n}{x} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc2}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
                    \PY{n}{x} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{dropout}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{training}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{training}\PY{p}{,} \PY{n}{p}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{)}
                    \PY{n}{x} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc3}\PY{p}{(}\PY{n}{x}\PY{p}{)}
                    
                \PY{k}{return} \PY{n}{x}
\end{Verbatim}


    \hypertarget{learning-agent}{%
\subsection{3. Learning Agent}\label{learning-agent}}

The DQN algorithm as described in the Deep Mind papers cited above has
the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialize parameters for Q(s, a) and Qˆ(s, a) with random weights, ε
  ← 1.0, and empty replay buffer
\item
  With probability ε, select a random action a, otherwise a = arg maxa
  Qs,a
\item
  Execute action a in an emulator and observe reward r and the next
  state s′
\item
  Store transition (s, a, r, s′) in the replay buffer
\item
  Sample a random minibatch of transitions from the replay buffer
\item
  For every transition in the buffer, calculate target y = r if the
  episode has ended at this step or y = r + γ maxa′∈A Qˆs′,a′ otherwise
\item
  Calculate loss: L = (Qs,a − y)2
\item
  Update Q(s, a) using the SGD algorithm by minimizing the loss in
  respect to model parameters
\item
  Every N steps copy weights from Q to Qt
\item
  Repeat from step 2 until converged
\end{enumerate}

These steps are implemented below.

    \hypertarget{a-define-the-hyperparameters}{%
\subsubsection{a) Define the
hyperparameters}\label{a-define-the-hyperparameters}}

\begin{verbatim}
- BUFFER_SIZE is the maximum capacity of the replay buffer;
- BATCH_SIZE is the size of batches sampled from the replay buffer;
- GAMMA is the discount used for the Bellman approximation;
- LR is the learning rate for the Neural Network;
- UPDATE_EVERY is the # of time steps to store experience before a learning step.
\end{verbatim}

The above hyperparameters are unchanged from some sample code provided
in a previous lunar lander assignment. I did try changing some of these
hyperparameters (e.g., increasing learning rate, etc.) but found it
unhelpful for speeding up convergence with the goal of this assignment.

I didn't see ``Tau'' (which was used as a hyperparameter in the starter
code to update target parameters) mentioned in the Deep Mind papers but
did notice it used in other literature.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n}{BUFFER\PYZus{}SIZE} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{l+m+mf}{1e5}\PY{p}{)}  \PY{c+c1}{\PYZsh{} replay buffer size}
        \PY{n}{BATCH\PYZus{}SIZE} \PY{o}{=} \PY{l+m+mi}{64}         \PY{c+c1}{\PYZsh{} minibatch size}
        \PY{n}{GAMMA} \PY{o}{=} \PY{l+m+mf}{0.99}            \PY{c+c1}{\PYZsh{} discount factor}
        \PY{n}{TAU} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}3}              \PY{c+c1}{\PYZsh{} for soft update of target parameters}
        \PY{n}{LR} \PY{o}{=} \PY{l+m+mf}{5e\PYZhy{}4}               \PY{c+c1}{\PYZsh{} learning rate }
        \PY{n}{UPDATE\PYZus{}EVERY} \PY{o}{=} \PY{l+m+mi}{4}        \PY{c+c1}{\PYZsh{} how often to update the network}
\end{Verbatim}


    \hypertarget{b-set-up-agent-to-interact-and-learn-from-environment}{%
\subsubsection{b) Set up Agent to interact and learn from
environment}\label{b-set-up-agent-to-interact-and-learn-from-environment}}

First, we set up an agent which will interact and learn from the
environment and interact with the deep Q network.

The agent stores references to the environment and experience replay
buffer (set up below). The agent is also used to track the current
observation and total accumulated reward.

In the ``act'' function, the agent selects an action. With probability
epsilon a random action is taken or, otherwise, actions are taken based
on maximum Q values from the past model. As actions are taken, they are
passed to the environment to get the next observation and reward, store
the data in the experience buffer and handle the end-of-episode.

\hypertarget{double-dqn}{%
\subsubsection{• Double DQN}\label{double-dqn}}

DQN tends to overestimate Q values because max operator in the Q
learning equation uses the same value for both selecting and evaluating
an action. This means that random noise will lead to overestimations in
Q values. This is solved by having two separate Q functions, each
learning independently. One Q function is used to select an action and
the other Q function is used to evaluate an action.

An option to apply Double DQN is implemented in the Agent class below.
If a Double DQN is applied, then a separate Q function is used to select
and evaluate actions.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{k}{class} \PY{n+nc}{Agent}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Interacts with and learns from the environment.\PYZdq{}\PYZdq{}\PYZdq{}}
        
            \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{state\PYZus{}size}\PY{p}{,} \PY{n}{action\PYZus{}size}\PY{p}{,} \PY{n}{seed}\PY{p}{,} \PY{n}{double}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{dueling}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Initialize an Agent object.}
        \PY{l+s+sd}{        }
        \PY{l+s+sd}{        Params}
        \PY{l+s+sd}{        ======}
        \PY{l+s+sd}{            state\PYZus{}size (int): dimension of each state}
        \PY{l+s+sd}{            action\PYZus{}size (int): dimension of each action}
        \PY{l+s+sd}{            seed (int): random seed}
        \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{state\PYZus{}size} \PY{o}{=} \PY{n}{state\PYZus{}size}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{action\PYZus{}size} \PY{o}{=} \PY{n}{action\PYZus{}size}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{seed} \PY{o}{=} \PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{n}{seed}\PY{p}{)}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{double} \PY{o}{=} \PY{n}{double}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dueling} \PY{o}{=} \PY{n}{dueling}
        
                \PY{c+c1}{\PYZsh{} Q\PYZhy{}Network}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{qnetwork\PYZus{}local} \PY{o}{=} \PY{n}{QNetwork}\PY{p}{(}\PY{n}{state\PYZus{}size}\PY{p}{,} \PY{n}{action\PYZus{}size}\PY{p}{,} \PY{n}{seed}\PY{p}{,} \PY{n}{dueling}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dueling}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{qnetwork\PYZus{}target} \PY{o}{=} \PY{n}{QNetwork}\PY{p}{(}\PY{n}{state\PYZus{}size}\PY{p}{,} \PY{n}{action\PYZus{}size}\PY{p}{,} \PY{n}{seed}\PY{p}{,} \PY{n}{dueling}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dueling}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{optimizer} \PY{o}{=} \PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{qnetwork\PYZus{}local}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{n}{LR}\PY{p}{)}
        
                \PY{c+c1}{\PYZsh{} Replay memory}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{memory} \PY{o}{=} \PY{n}{ReplayBuffer}\PY{p}{(}\PY{n}{action\PYZus{}size}\PY{p}{,} \PY{n}{BUFFER\PYZus{}SIZE}\PY{p}{,} \PY{n}{BATCH\PYZus{}SIZE}\PY{p}{,} \PY{n}{seed}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} Initialize time step (for updating every UPDATE\PYZus{}EVERY steps)}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{t\PYZus{}step} \PY{o}{=} \PY{l+m+mi}{0}
            
            \PY{k}{def} \PY{n+nf}{step}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{state}\PY{p}{,} \PY{n}{action}\PY{p}{,} \PY{n}{reward}\PY{p}{,} \PY{n}{next\PYZus{}state}\PY{p}{,} \PY{n}{done}\PY{p}{)}\PY{p}{:}
                \PY{c+c1}{\PYZsh{} Save experience in replay memory}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{memory}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{state}\PY{p}{,} \PY{n}{action}\PY{p}{,} \PY{n}{reward}\PY{p}{,} \PY{n}{next\PYZus{}state}\PY{p}{,} \PY{n}{done}\PY{p}{)}
                
                \PY{c+c1}{\PYZsh{} Learn every UPDATE\PYZus{}EVERY time steps.}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{t\PYZus{}step} \PY{o}{=} \PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{t\PYZus{}step} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{\PYZpc{}} \PY{n}{UPDATE\PYZus{}EVERY}
                \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{t\PYZus{}step} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                    \PY{c+c1}{\PYZsh{} If enough samples are available in memory, get random subset and learn}
                    \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{memory}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{n}{BATCH\PYZus{}SIZE}\PY{p}{:}
                        \PY{n}{experiences} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{memory}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{p}{)}
                        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{learn}\PY{p}{(}\PY{n}{experiences}\PY{p}{,} \PY{n}{GAMMA}\PY{p}{)}
        
            \PY{k}{def} \PY{n+nf}{act}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{state}\PY{p}{,} \PY{n}{eps}\PY{o}{=}\PY{l+m+mf}{0.}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Returns actions for given state as per current policy.}
        \PY{l+s+sd}{        }
        \PY{l+s+sd}{        Params}
        \PY{l+s+sd}{        ======}
        \PY{l+s+sd}{            state (array\PYZus{}like): current state}
        \PY{l+s+sd}{            eps (float): epsilon, for epsilon\PYZhy{}greedy action selection}
        \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{n}{state} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{from\PYZus{}numpy}\PY{p}{(}\PY{n}{state}\PY{p}{)}\PY{o}{.}\PY{n}{float}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{qnetwork\PYZus{}local}\PY{o}{.}\PY{n}{eval}\PY{p}{(}\PY{p}{)}
                \PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                    \PY{n}{action\PYZus{}values} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{qnetwork\PYZus{}local}\PY{p}{(}\PY{n}{state}\PY{p}{)}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{qnetwork\PYZus{}local}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{p}{)}
        
                \PY{c+c1}{\PYZsh{} Epsilon\PYZhy{}greedy action selection}
                \PY{k}{if} \PY{n}{random}\PY{o}{.}\PY{n}{random}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{n}{eps}\PY{p}{:}
                    \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{action\PYZus{}values}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{)}
                \PY{k}{else}\PY{p}{:}
                    \PY{k}{return} \PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{action\PYZus{}size}\PY{p}{)}\PY{p}{)}
        
            \PY{k}{def} \PY{n+nf}{learn}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{experiences}\PY{p}{,} \PY{n}{gamma}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Update value parameters using given batch of experience tuples.}
        
        \PY{l+s+sd}{        Params}
        \PY{l+s+sd}{        ======}
        \PY{l+s+sd}{            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s\PYZsq{}, done) tuples }
        \PY{l+s+sd}{            gamma (float): discount factor}
        \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{n}{states}\PY{p}{,} \PY{n}{actions}\PY{p}{,} \PY{n}{rewards}\PY{p}{,} \PY{n}{next\PYZus{}states}\PY{p}{,} \PY{n}{dones} \PY{o}{=} \PY{n}{experiences}
        
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{        DQN v. Double DQN}
        \PY{l+s+sd}{        =================}
        \PY{l+s+sd}{        Double DQN\PYZhy{} Calculate the best action to take in the next state }
        \PY{l+s+sd}{            using main/local network}
        \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}        
                \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{double}\PY{p}{:}
                    \PY{c+c1}{\PYZsh{} Q target based on local rather than target model}
                    \PY{n}{next\PYZus{}state\PYZus{}actions} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{qnetwork\PYZus{}local}\PY{p}{(}\PY{n}{next\PYZus{}states}\PY{p}{)}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
                    \PY{n}{Q\PYZus{}targets\PYZus{}next} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{qnetwork\PYZus{}target}\PY{p}{(}\PY{n}{next\PYZus{}states}\PY{p}{)}\PY{o}{.}\PY{n}{gather}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{next\PYZus{}state\PYZus{}actions}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
                \PY{k}{else}\PY{p}{:}
                        \PY{c+c1}{\PYZsh{} DQN \PYZhy{} Get max predicted Q values (for next states) from target model}
                        \PY{n}{Q\PYZus{}targets\PYZus{}next} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{qnetwork\PYZus{}target}\PY{p}{(}\PY{n}{next\PYZus{}states}\PY{p}{)}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
        
                \PY{c+c1}{\PYZsh{} Compute Q targets for current states }
                \PY{n}{Q\PYZus{}targets} \PY{o}{=} \PY{n}{rewards} \PY{o}{+} \PY{p}{(}\PY{n}{gamma} \PY{o}{*} \PY{n}{Q\PYZus{}targets\PYZus{}next} \PY{o}{*} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{dones}\PY{p}{)}\PY{p}{)}
        
                \PY{c+c1}{\PYZsh{} Get expected Q values from local model}
                \PY{n}{Q\PYZus{}expected} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{qnetwork\PYZus{}local}\PY{p}{(}\PY{n}{states}\PY{p}{)}\PY{o}{.}\PY{n}{gather}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{actions}\PY{p}{)}
        
                \PY{c+c1}{\PYZsh{} Compute loss}
                \PY{n}{loss} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{mse\PYZus{}loss}\PY{p}{(}\PY{n}{Q\PYZus{}expected}\PY{p}{,} \PY{n}{Q\PYZus{}targets}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} Minimize the loss}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{optimizer}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
                \PY{n}{loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{optimizer}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}
        
                \PY{c+c1}{\PYZsh{} \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{} update target network \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{} \PYZsh{}}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{soft\PYZus{}update}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{qnetwork\PYZus{}local}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{qnetwork\PYZus{}target}\PY{p}{,} \PY{n}{TAU}\PY{p}{)}                     
        
            \PY{k}{def} \PY{n+nf}{soft\PYZus{}update}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{local\PYZus{}model}\PY{p}{,} \PY{n}{target\PYZus{}model}\PY{p}{,} \PY{n}{tau}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Soft update model parameters.}
        \PY{l+s+sd}{        θ\PYZus{}target = τ*θ\PYZus{}local + (1 \PYZhy{} τ)*θ\PYZus{}target}
        
        \PY{l+s+sd}{        Params}
        \PY{l+s+sd}{        ======}
        \PY{l+s+sd}{            local\PYZus{}model (PyTorch model): weights will be copied from}
        \PY{l+s+sd}{            target\PYZus{}model (PyTorch model): weights will be copied to}
        \PY{l+s+sd}{            tau (float): interpolation parameter }
        \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{k}{for} \PY{n}{target\PYZus{}param}\PY{p}{,} \PY{n}{local\PYZus{}param} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{target\PYZus{}model}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{local\PYZus{}model}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                    \PY{n}{target\PYZus{}param}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{copy\PYZus{}}\PY{p}{(}\PY{n}{tau}\PY{o}{*}\PY{n}{local\PYZus{}param}\PY{o}{.}\PY{n}{data} \PY{o}{+} \PY{p}{(}\PY{l+m+mf}{1.0}\PY{o}{\PYZhy{}}\PY{n}{tau}\PY{p}{)}\PY{o}{*}\PY{n}{target\PYZus{}param}\PY{o}{.}\PY{n}{data}\PY{p}{)}
\end{Verbatim}


    \hypertarget{c-set-up-experience-replay-buffer-to-store-experiences-to-learn-from}{%
\subsubsection{c) Set up Experience Replay Buffer to store experiences
to learn
from}\label{c-set-up-experience-replay-buffer-to-store-experiences-to-learn-from}}

The purpose of the experience replay buffer is to store the transition
experience obtained from the environment (tuples of observation, action,
reward, done flag, and next state). Each time we step through the
environment, the experience is stored in the buffer, keeping only a
fixed number of steps.

For training, we randomly sample the batch of transition experiences
from the replay buffer. By sampling randomly, we break any correlation
between subsequent steps in an environment

    If I were to set up a Prioritized Replay Buffer (one of the Rainbow of
improvements described above), the steps would be as follows:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Replay Buffer needs to be modified to:

  \begin{itemize}
  \tightlist
  \item
    track priorities;
  \item
    sample a batch according to them;
  \item
    calculate weights; and
  \item
    allow priorities to be updated after the loss has become known.
  \end{itemize}
\item
  Loss Function has to modified:

  \begin{itemize}
  \tightlist
  \item
    to incorporate weights for every sample, but pass loss values back
    to the replay buffer to adjust the priorities of sampled
    transitions.
  \end{itemize}
\end{enumerate}

Given time constraints, I did not implement the above in this
assignment.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k}{class} \PY{n+nc}{ReplayBuffer}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Fixed\PYZhy{}size buffer to store experience tuples.\PYZdq{}\PYZdq{}\PYZdq{}}
        
            \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{action\PYZus{}size}\PY{p}{,} \PY{n}{buffer\PYZus{}size}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{seed}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Initialize a ReplayBuffer object.}
        
        \PY{l+s+sd}{        Params}
        \PY{l+s+sd}{        ======}
        \PY{l+s+sd}{            action\PYZus{}size (int): dimension of each action}
        \PY{l+s+sd}{            buffer\PYZus{}size (int): maximum size of buffer}
        \PY{l+s+sd}{            batch\PYZus{}size (int): size of each training batch}
        \PY{l+s+sd}{            seed (int): random seed}
        \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{action\PYZus{}size} \PY{o}{=} \PY{n}{action\PYZus{}size}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{memory} \PY{o}{=} \PY{n}{deque}\PY{p}{(}\PY{n}{maxlen}\PY{o}{=}\PY{n}{buffer\PYZus{}size}\PY{p}{)}  
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{n}{batch\PYZus{}size}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{experience} \PY{o}{=} \PY{n}{namedtuple}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Experience}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{field\PYZus{}names}\PY{o}{=}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{state}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{action}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{reward}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{next\PYZus{}state}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{done}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{seed} \PY{o}{=} \PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{n}{seed}\PY{p}{)}
            
            \PY{k}{def} \PY{n+nf}{add}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{state}\PY{p}{,} \PY{n}{action}\PY{p}{,} \PY{n}{reward}\PY{p}{,} \PY{n}{next\PYZus{}state}\PY{p}{,} \PY{n}{done}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Add a new experience to memory.\PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{n}{e} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{experience}\PY{p}{(}\PY{n}{state}\PY{p}{,} \PY{n}{action}\PY{p}{,} \PY{n}{reward}\PY{p}{,} \PY{n}{next\PYZus{}state}\PY{p}{,} \PY{n}{done}\PY{p}{)}
                \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{memory}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{e}\PY{p}{)}
            
            \PY{k}{def} \PY{n+nf}{sample}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Randomly sample a batch of experiences from memory.\PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{n}{experiences} \PY{o}{=} \PY{n}{random}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{memory}\PY{p}{,} \PY{n}{k}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{batch\PYZus{}size}\PY{p}{)}
        
                \PY{n}{states} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{from\PYZus{}numpy}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{p}{[}\PY{n}{e}\PY{o}{.}\PY{n}{state} \PY{k}{for} \PY{n}{e} \PY{o+ow}{in} \PY{n}{experiences} \PY{k}{if} \PY{n}{e} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{float}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
                \PY{n}{actions} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{from\PYZus{}numpy}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{p}{[}\PY{n}{e}\PY{o}{.}\PY{n}{action} \PY{k}{for} \PY{n}{e} \PY{o+ow}{in} \PY{n}{experiences} \PY{k}{if} \PY{n}{e} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{long}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
                \PY{n}{rewards} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{from\PYZus{}numpy}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{p}{[}\PY{n}{e}\PY{o}{.}\PY{n}{reward} \PY{k}{for} \PY{n}{e} \PY{o+ow}{in} \PY{n}{experiences} \PY{k}{if} \PY{n}{e} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{float}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
                \PY{n}{next\PYZus{}states} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{from\PYZus{}numpy}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{p}{[}\PY{n}{e}\PY{o}{.}\PY{n}{next\PYZus{}state} \PY{k}{for} \PY{n}{e} \PY{o+ow}{in} \PY{n}{experiences} \PY{k}{if} \PY{n}{e} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{float}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
                \PY{n}{dones} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{from\PYZus{}numpy}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{p}{[}\PY{n}{e}\PY{o}{.}\PY{n}{done} \PY{k}{for} \PY{n}{e} \PY{o+ow}{in} \PY{n}{experiences} \PY{k}{if} \PY{n}{e} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{uint8}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{float}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
          
                \PY{k}{return} \PY{p}{(}\PY{n}{states}\PY{p}{,} \PY{n}{actions}\PY{p}{,} \PY{n}{rewards}\PY{p}{,} \PY{n}{next\PYZus{}states}\PY{p}{,} \PY{n}{dones}\PY{p}{)}
        
            \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}len\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Return the current size of internal memory.\PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{k}{return} \PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{memory}\PY{p}{)}
\end{Verbatim}


    \hypertarget{d-set-up-training-loop-and-output-and-save-progress}{%
\subsubsection{d) Set up training loop and output and save
progress}\label{d-set-up-training-loop-and-output-and-save-progress}}

The DQN function is used to loop through observation, training, and
learning steps while tracking output and saving progress.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{k}{def} \PY{n+nf}{dqn}\PY{p}{(}\PY{n}{n\PYZus{}episodes}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{,} \PY{n}{max\PYZus{}t}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{,} \PY{n}{eps\PYZus{}start}\PY{o}{=}\PY{l+m+mf}{1.0}\PY{p}{,} \PY{n}{eps\PYZus{}end}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{,} \PY{n}{eps\PYZus{}decay}\PY{o}{=}\PY{l+m+mf}{0.995}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Deep Q\PYZhy{}Learning.}
        \PY{l+s+sd}{    }
        \PY{l+s+sd}{    Params}
        \PY{l+s+sd}{    ======}
        \PY{l+s+sd}{        n\PYZus{}episodes (int): maximum number of training episodes}
        \PY{l+s+sd}{        max\PYZus{}t (int): maximum number of timesteps per episode}
        \PY{l+s+sd}{        eps\PYZus{}start (float): starting value of epsilon, for epsilon\PYZhy{}greedy action selection}
        \PY{l+s+sd}{        eps\PYZus{}end (float): minimum value of epsilon}
        \PY{l+s+sd}{        eps\PYZus{}decay (float): multiplicative factor (per episode) for decreasing epsilon}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{n}{scores} \PY{o}{=} \PY{p}{[}\PY{p}{]}                        \PY{c+c1}{\PYZsh{} list containing scores from each episode}
            \PY{n}{scores\PYZus{}window} \PY{o}{=} \PY{n}{deque}\PY{p}{(}\PY{n}{maxlen}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}  \PY{c+c1}{\PYZsh{} last 100 scores}
            \PY{n}{eps} \PY{o}{=} \PY{n}{eps\PYZus{}start}                    \PY{c+c1}{\PYZsh{} initialize epsilon}
            \PY{n}{solved} \PY{o}{=} \PY{k+kc}{False}
              
            \PY{k}{for} \PY{n}{i\PYZus{}episode} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{n\PYZus{}episodes}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
                \PY{c+c1}{\PYZsh{} Reset environment for training}
                \PY{n}{env\PYZus{}info} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{reset}\PY{p}{(}\PY{n}{train\PYZus{}mode}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{[}\PY{n}{brain\PYZus{}name}\PY{p}{]}
                \PY{n}{state} \PY{o}{=} \PY{n}{env\PYZus{}info}\PY{o}{.}\PY{n}{vector\PYZus{}observations}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} 
                
                \PY{n}{score} \PY{o}{=} \PY{l+m+mi}{0}
                \PY{k}{for} \PY{n}{t} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{max\PYZus{}t}\PY{p}{)}\PY{p}{:}
                    
                    \PY{c+c1}{\PYZsh{} Select and perform an action}
                    \PY{n}{action} \PY{o}{=} \PY{n}{agent}\PY{o}{.}\PY{n}{act}\PY{p}{(}\PY{n}{state}\PY{p}{,} \PY{n}{eps}\PY{p}{)}
                    \PY{n}{env\PYZus{}info} \PY{o}{=} \PY{n}{env}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{n}{action}\PY{p}{)}\PY{p}{[}\PY{n}{brain\PYZus{}name}\PY{p}{]}
                    
                    \PY{c+c1}{\PYZsh{} get the next state}
                    \PY{n}{next\PYZus{}state} \PY{o}{=} \PY{n}{env\PYZus{}info}\PY{o}{.}\PY{n}{vector\PYZus{}observations}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
                    
                    \PY{c+c1}{\PYZsh{} get the reward}
                    \PY{n}{reward} \PY{o}{=} \PY{n}{env\PYZus{}info}\PY{o}{.}\PY{n}{rewards}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
                    
                    \PY{c+c1}{\PYZsh{} see if episode has finished}
                    \PY{n}{done} \PY{o}{=} \PY{n}{env\PYZus{}info}\PY{o}{.}\PY{n}{local\PYZus{}done}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
                    
                    \PY{c+c1}{\PYZsh{} update the score; roll over the state to next time step; exit loop if episode finished}
                    \PY{n}{agent}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{n}{state}\PY{p}{,} \PY{n}{action}\PY{p}{,} \PY{n}{reward}\PY{p}{,} \PY{n}{next\PYZus{}state}\PY{p}{,} \PY{n}{done}\PY{p}{)}
                    \PY{n}{state} \PY{o}{=} \PY{n}{next\PYZus{}state}
                    \PY{n}{score} \PY{o}{+}\PY{o}{=} \PY{n}{reward}
                    \PY{k}{if} \PY{n}{done}\PY{p}{:}
                        \PY{k}{break} 
                
                \PY{c+c1}{\PYZsh{} save most recent score and decrease epsilon}
                \PY{n}{scores\PYZus{}window}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{score}\PY{p}{)}
                \PY{n}{scores}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{score}\PY{p}{)}
                \PY{n}{eps} \PY{o}{=} \PY{n+nb}{max}\PY{p}{(}\PY{n}{eps\PYZus{}end}\PY{p}{,} \PY{n}{eps\PYZus{}decay}\PY{o}{*}\PY{n}{eps}\PY{p}{)}
                
                \PY{c+c1}{\PYZsh{} output progress and save model}
                \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}r}\PY{l+s+s1}{Episode }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{Average Score: }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{Latest Score: }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{i\PYZus{}episode}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{scores\PYZus{}window}\PY{p}{)}\PY{p}{,} \PY{n}{score}\PY{p}{)}\PY{p}{,} \PY{n}{end}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                \PY{k}{if} \PY{n}{i\PYZus{}episode} \PY{o}{\PYZpc{}} \PY{l+m+mi}{100} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}r}\PY{l+s+s1}{Episode }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{Average Score: }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{i\PYZus{}episode}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{scores\PYZus{}window}\PY{p}{)}\PY{p}{)}\PY{p}{)}
                \PY{k}{if} \PY{o+ow}{not} \PY{n}{solved}\PY{p}{:}
                    \PY{k}{if} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{scores\PYZus{}window}\PY{p}{)}\PY{o}{\PYZgt{}}\PY{o}{=}\PY{l+m+mf}{13.0}\PY{p}{:}
                        \PY{n}{solved} \PY{o}{=} \PY{k+kc}{True}
                        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{Environment solved in }\PY{l+s+si}{\PYZob{}:d\PYZcb{}}\PY{l+s+s1}{ episodes!}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{Average Score: }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{i\PYZus{}episode}\PY{o}{\PYZhy{}}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{scores\PYZus{}window}\PY{p}{)}\PY{p}{)}\PY{p}{)}
                        \PY{n}{torch}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{n}{agent}\PY{o}{.}\PY{n}{qnetwork\PYZus{}local}\PY{o}{.}\PY{n}{state\PYZus{}dict}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{checkpoint.pth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{k}{return} \PY{n}{scores}
\end{Verbatim}


    \hypertarget{run-the-algorithm-and-plot-the-outputs}{%
\subsection{4. Run the algorithm and plot the
outputs}\label{run-the-algorithm-and-plot-the-outputs}}

Okay. Let's do this!

I will run the model 4 times utilizing 4 different scenarios:

\begin{verbatim}
a)  Plain vanilla DQN
b)  Double DQN
c)  Dueling DQN
d)  Combination of Double DQN and Dueling DQN
\end{verbatim}

    \hypertarget{a-plain-vanilla-dqn}{%
\subsubsection{a) Plain vanilla DQN}\label{a-plain-vanilla-dqn}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n}{agent} \PY{o}{=} \PY{n}{Agent}\PY{p}{(}\PY{n}{state\PYZus{}size}\PY{o}{=}\PY{n}{state\PYZus{}size}\PY{p}{,} \PY{n}{action\PYZus{}size}\PY{o}{=}\PY{n}{action\PYZus{}size}\PY{p}{,} \PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} 
                      \PY{n}{double}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{dueling}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Train the network}
        \PY{n}{scores} \PY{o}{=} \PY{n}{dqn}\PY{p}{(}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} plot the scores}
        \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
        \PY{n}{ax} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{111}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{scores}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{scores}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Episode \PYZsh{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Episode 100	Average Score: 0.64	Latest Score: 3.000
Episode 200	Average Score: 4.50	Latest Score: 6.000
Episode 300	Average Score: 8.97	Latest Score: 4.000
Episode 400	Average Score: 11.68	Latest Score: 15.00
Episode 450	Average Score: 13.00	Latest Score: 11.00
Environment solved in 350 episodes!	Average Score: 13.00
Episode 500	Average Score: 13.79	Latest Score: 9.000
Episode 600	Average Score: 14.56	Latest Score: 16.00
Episode 700	Average Score: 14.73	Latest Score: 11.00
Episode 800	Average Score: 15.73	Latest Score: 11.00
Episode 900	Average Score: 15.44	Latest Score: 11.00
Episode 1000	Average Score: 16.09	Latest Score: 14.00

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_17_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{vanilla-dqn-notes}{%
\subparagraph{Vanilla DQN Notes:}\label{vanilla-dqn-notes}}

Good news. Task solved. The basic DQN model works.

    \hypertarget{b-double-dqn}{%
\subsubsection{b) Double DQN}\label{b-double-dqn}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{n}{agent} \PY{o}{=} \PY{n}{Agent}\PY{p}{(}\PY{n}{state\PYZus{}size}\PY{o}{=}\PY{n}{state\PYZus{}size}\PY{p}{,} \PY{n}{action\PYZus{}size}\PY{o}{=}\PY{n}{action\PYZus{}size}\PY{p}{,} \PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} 
                      \PY{n}{double}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{dueling}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Train the network}
        \PY{n}{scores} \PY{o}{=} \PY{n}{dqn}\PY{p}{(}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} plot the scores}
        \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
        \PY{n}{ax} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{111}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{scores}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{scores}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Episode \PYZsh{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Episode 100	Average Score: -0.12	Latest Score: 0.00
Episode 200	Average Score: 3.25	Latest Score: 7.000
Episode 300	Average Score: 7.86	Latest Score: 11.00
Episode 400	Average Score: 9.36	Latest Score: 2.000
Episode 500	Average Score: 12.68	Latest Score: 13.00
Episode 530	Average Score: 13.00	Latest Score: 18.00
Environment solved in 430 episodes!	Average Score: 13.00
Episode 600	Average Score: 14.22	Latest Score: 11.00
Episode 700	Average Score: 14.92	Latest Score: 15.00
Episode 800	Average Score: 15.22	Latest Score: 19.00
Episode 900	Average Score: 14.73	Latest Score: 18.00
Episode 1000	Average Score: 15.36	Latest Score: 15.00

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_20_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{double-dqn-notes}{%
\subparagraph{Double DQN Notes:}\label{double-dqn-notes}}

The Double DQN demonstrated somewhat better learning.

    \hypertarget{c-dueling-dqn}{%
\subsubsection{c) Dueling DQN}\label{c-dueling-dqn}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{n}{agent} \PY{o}{=} \PY{n}{Agent}\PY{p}{(}\PY{n}{state\PYZus{}size}\PY{o}{=}\PY{n}{state\PYZus{}size}\PY{p}{,} \PY{n}{action\PYZus{}size}\PY{o}{=}\PY{n}{action\PYZus{}size}\PY{p}{,} \PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} 
                      \PY{n}{double}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{dueling}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Train the network}
        \PY{n}{scores} \PY{o}{=} \PY{n}{dqn}\PY{p}{(}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} plot the scores}
        \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
        \PY{n}{ax} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{111}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{scores}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{scores}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Episode \PYZsh{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Episode 100	Average Score: 0.85	Latest Score: -1.00
Episode 200	Average Score: 4.03	Latest Score: 4.000
Episode 300	Average Score: 7.72	Latest Score: 13.00
Episode 400	Average Score: 12.05	Latest Score: 13.00
Episode 441	Average Score: 13.02	Latest Score: 19.00
Environment solved in 341 episodes!	Average Score: 13.02
Episode 500	Average Score: 14.27	Latest Score: 16.00
Episode 600	Average Score: 14.74	Latest Score: 13.00
Episode 700	Average Score: 15.10	Latest Score: 17.00
Episode 800	Average Score: 14.62	Latest Score: 18.00
Episode 900	Average Score: 14.67	Latest Score: 17.00
Episode 1000	Average Score: 14.46	Latest Score: 24.00

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_23_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{dueling-dqn-notes}{%
\subparagraph{Dueling DQN Notes:}\label{dueling-dqn-notes}}

My Dueling DQN implementation seems to not have performed as well as
expected (compared to Google Deep Mind paper) but does converge.

    \hypertarget{d-combined-dueling-double-dqn}{%
\subsubsection{d) Combined Dueling Double
DQN}\label{d-combined-dueling-double-dqn}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{n}{agent} \PY{o}{=} \PY{n}{Agent}\PY{p}{(}\PY{n}{state\PYZus{}size}\PY{o}{=}\PY{n}{state\PYZus{}size}\PY{p}{,} \PY{n}{action\PYZus{}size}\PY{o}{=}\PY{n}{action\PYZus{}size}\PY{p}{,} \PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} 
                       \PY{n}{double}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{dueling}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Train the network}
         \PY{n}{scores} \PY{o}{=} \PY{n}{dqn}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} plot the scores}
         \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
         \PY{n}{ax} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{111}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{scores}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{scores}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Episode \PYZsh{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Episode 100	Average Score: 0.42	Latest Score: 3.000
Episode 200	Average Score: 4.03	Latest Score: 11.00
Episode 300	Average Score: 7.91	Latest Score: 7.000
Episode 400	Average Score: 9.88	Latest Score: 9.000
Episode 498	Average Score: 13.06	Latest Score: 22.00
Environment solved in 398 episodes!	Average Score: 13.06
Episode 500	Average Score: 13.07	Latest Score: 13.00
Episode 600	Average Score: 14.92	Latest Score: 15.00
Episode 700	Average Score: 14.82	Latest Score: 21.00
Episode 800	Average Score: 15.79	Latest Score: 14.00
Episode 900	Average Score: 15.42	Latest Score: 18.00
Episode 1000	Average Score: 15.63	Latest Score: 22.00

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_26_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{combined-dueling-double-dqn-notes}{%
\subparagraph{Combined Dueling Double DQN
Notes:}\label{combined-dueling-double-dqn-notes}}

Again, converges but without as much improvement compared to the Deep
Mind paper. I suspect my Dueling DQN implementation may need more work.
But assignment completed\ldots{}

    \hypertarget{further-work-to-be-considered}{%
\subsection{Further work to be
considered}\label{further-work-to-be-considered}}

If I had time (I don't!), I'd implement the remaining rainbow
improvements described above and would figure out how to tweak my
implementation of a dual network to make it work better. I would also
enjoy implementing a version of this algorithm that uses a deep
convolutional network to learn from game screens (e.g., just as Deep
Mind did in it's original papers).

In addition, would be great to utilize my final weights (sans drop out,
of course) to play the game. It's very possible that my aggressive use
of Drop Out, which reduces overfitting, may have subdued test results
compared to if I implement best weights achieved (without Dropout) to
play the game.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
